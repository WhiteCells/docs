### Java 环境

PySpark 需要 Java 运行环境。

```sh
sudo apt install openjdk-11-jdk
```

### Spark

```sh
wget https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
wget https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
```

```sh
mv spark-3.5.4 /usr/local/spark
```

修改 `shell` 配置文件：

```sh
export SPARK_HOME=/usr/local/spark
export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
export PYSPARK_PYTHON=$(which python3)
export PYSPARK_DRIVER_PYTHON=$(which python3)
```

### Hadoop

```sh
wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.4.1/hadoop-3.4.1.tar.gz
```

```sh
mv hadoop-3.4.1 /usr/local/hadoop
```

修改 `shell` 配置文件：

```sh
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```



修改 `/etc/hosts`

修改 `$HADOOP_HOME/etc/hadoop/` 下的 `core-site.xml` 及 `hdfs-site.xml` 文件



```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, max, min, count

spark = SparkSession.builder \
    .appName("PySpark") \
    .getOrCreate()

df = spark.read.csv('./test.csv', header=True, inferSchema=True)
print("origin data")
df.show()

print("show top 3 rows")
df.show(3)

# 列及属性
print("col attribute")
df.printSchema()

# 列选择
df_select = df.select("id", "name", "salary")
print("select col")
df_select.show()

# 过滤年龄大于35的人员
df_filtered_age_ge_35 = df.filter(df.age > 35)
print("filtered age greate 35")
df_filtered_age_ge_35.show()

# 排序
df_sorted_salary_desc = df.orderBy(col("salary").desc())
print("sorted salary desc")
df_sorted_salary_desc.show()

df_sorted_salary_asc = df.orderBy(col("salary").asc())
print("sorted salary asc")
df_sorted_salary_asc.show()

# 分组
df_grouped = df.groupBy("city").agg(
    avg("salary").alias("avg_salary")
)
print("grouped city avg salary")
df_grouped.show()

# 按城市分组，并计算最大、最小薪资、员工数
df_grouped_stats = df.groupBy("city").agg(
    max("salary").alias("max_salary"),
    min("salary").alias("min_salary"),
    count("salary").alias("num_employees")
)
print("grouped city stats")
df_grouped_stats.show()

# 使用 RDD 进行操作
rdd = df.rdd  # 转换为 RDD
rdd_filtered = rdd.filter(lambda row: row['age'] > 30)
print(rdd_filtered.collect())

# 使用 SQL 查询
df.createOrReplaceTempView("people")  # 注册临时视图
sql_result = spark.sql("SELECT name, salary FROM people WHERE age > 35 ORDER BY salary DESC")
print("select age greate 35")
sql_result.show()

# 数据合并
df1 = spark.read.csv('test.csv', header=True, inferSchema=True)
df_combined = df.union(df1)
print("union")
df_combined.show()

df.write.csv('output.csv', header=True)

spark.stop()
```

